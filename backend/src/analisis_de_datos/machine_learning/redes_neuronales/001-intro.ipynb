{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Introducción a las redes neuronales </center>\n",
    "Las redes neuronales (también conocidas como sistemas conexionistas) son un modelo computacional basado en un gran conjunto de unidades neuronales simples (neuronas artificiales), de forma aproximadamente análoga al comportamiento observado en los axones de las neuronas en los cerebros biológicos.\n",
    "\n",
    "![Red neuronal](https://s20.postimg.org/q0t1jzcxp/Colored_neural_network.png)\n",
    "\n",
    "Las redes neuronales se componen de unas entradas (inputs), unas capas intermedias de procesamiento (hidden layers) y unas salidas (outputs). Cada conexión a un nodo está equipado con un peso, por el cual se multiplica el valor de entrada para evaluarlo dentro del nodo. \n",
    "\n",
    " En la siguiente imagen podemos ver los pesos de las conexiones en una red neuronal muy simple. Esto daría como resultado una multiplicación en forma de matrix de la siguiente forma:\n",
    " \n",
    "$$ \\begin{bmatrix}w_{12} & w_{21}\\\\w_{12} & w_{22}\\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "\n",
    "Para más información sobre matrices consultar el [notebook sobre algebra lineal](https://github.com/mondeja/fullstack/blob/master/backend/src/001-matematicas/algebra_aritmetica/004-algebra_lineal/matrices.ipynb).\n",
    "\n",
    "![Matrix red neuronal](https://s20.postimg.org/z50nbv3jx/matrix_neuronal.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales son como los algoritmos, hay una caja negra con un número de entradas que produce unas salidas. Pero hay algo especial en esa caja que representa las redes neuronales: una serie de controles analógicos, algunos girando a la izquierda y otros girando a la derecha de tal modo que su giro afecta las salidas:\n",
    "\n",
    "![caja redes neuronales](https://s20.postimg.org/kynkdr7lp/caja_negra_redes_neuronales.png)\n",
    "\n",
    "La cuestión radica en encontrar los valores de esas manecillas que darán las salidas deseadas para todos los casos en nuestro problema a resolver (o al menos para la gran mayoría de casos). Ese reto puede dar lugar a sistemas de gran complejidad.\n",
    "\n",
    "_________________________________________________________\n",
    "\n",
    "## Perceptrones simples\n",
    "Para dar inicio con las redes neuronales se parte de lo más simple: una neurona. Se le conoce como perceptrón simple. Se presenta, simplificadamente, así:\n",
    "\n",
    "![perceptron simple](https://s20.postimg.org/e8734gxcd/perceptron_simple.png)\n",
    "\n",
    "> En el [siguiente capítulo](https://github.com/mondeja/fullstack/blob/master/backend/src/analisis_de_datos/machine_learning/redes_neuronales/002-perceptron.ipynb) se toca el tema de los perceptrones en mayor profundidad, pero como puede ser bastante difícil de digerir así de primeras, empecemos con algo más intuitivo para entenderlo luego en profundidad.\n",
    "\n",
    "### Implementando perceptrones simples en Python\n",
    " \n",
    "Vamos a hacer que una neurona aprenda las tablas AND y OR de lógica booleana:\n",
    "\n",
    "|  **AND**  | True  | False |\n",
    "|-----------|:-----:|:-----:|\n",
    "| **True**  | True  | False |\n",
    "| **False** | False | False |\n",
    "\n",
    "|  **OR**   | True  | False |\n",
    "|-----------|:-----:|:-----:|\n",
    "| **True**  | True  | True  |\n",
    "| **False** | True  | False |\n",
    "\n",
    "Llamaremos 1 a verdadero y 0 a Falso. Es una buena elección acostumbrarse a la notación numérica de estos valores ya que su manejo resulta más eficiente en las bibliotecas y con lenguajes a más bajo nivel como C son la única posibilidad.\n",
    "\n",
    "#### Elementos básicos de una neurona artificial\n",
    "\n",
    "Los elementos esenciales de una neurona son los siguientes:\n",
    "- Entrada/s.\n",
    "- Peso/s de las entrada/s.\n",
    "- Función de activación.\n",
    "- Umbral de dicha función.\n",
    "- Salida/s.\n",
    "\n",
    "Por cada entrada (0 ó 1 en este caso), la neurona empieza con un peso cuyo valor comienza al azar. El proceso que sigue es el siguiente:\n",
    "1. Toma las entradas y multiplica cada una por su peso. Este proceso de multiplicación obliga al dato a tomar más o menos relevancia en el resultado, por eso se denomina **peso**. Todas las entradas pesadas se suman para obtener un valor, el cual se multiplica por el peso del umbral lo que provoca la ecuación para este caso: $E_1 \\cdot P_1 + E_2 \\cdot P_2 \\cdot P_U$\n",
    "2.  Este valor se pasa a la función de activación, en la cual será procesado para obtener un valor. El umbral es el cual se encarga de producir una salida binaria (o se activa o no se activa) dependiendo de si el valor que hemos obtenido lo supera o no.\n",
    "3. Si la salida coincide con el resultado esperado, podríamos decir que ha aprendido, pero si no, los pesos se recalculan para seguir aprendiendo. Pero ¿cómo vamos a recalcular los pesos para que se inclinen hacia donde deseeamos para que en la próxima iteración el valor obtenido se asemeje más al objetivo? Podríamos recalcularlos de forma aleatoria, pero esto haría que el proceso de aprendizaje fuera ciego y más costoso. Para solventar este problema usamos la fórmula matemática de Frank Rossenblatt:\n",
    "```\n",
    "Error = Salida Esperada – Salida Real\n",
    "Si Error != 0:\n",
    "    Nuevo Peso (entrada 1) = Peso anterior (entrada 1) + tasa aprendizaje * Error * Entrada 1\n",
    "    Nuevo Peso (entrada 2) = Peso anterior (entrada 2) + tasa aprendizaje * Error * Entrada 2\n",
    "    Nuevo Peso (umbral) = Peso anterior (de la entrada del umbral) + tasa aprendizaje * Error\n",
    "```\n",
    "La tasa de aprendizaje será un número decimal positivo muy pequeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABLA DE VERDAD AND\n",
      "Número de iteraciones necesarias: 22\n",
      "\n",
      "\n",
      "=======================================\n",
      "\n",
      "\n",
      "TABLA DE VERDAD OR\n",
      "Número de iteraciones necesarias: 13\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class PerceptronSimple:\n",
    "    \"\"\"Perceptrón simple que aprende tablas de lógica booleana\n",
    "    linealmente separables (activa el debug para observar su comportamiento\n",
    "    en cada iteración).\"\"\"\n",
    "    def __init__(self, tasa_aprendizaje, tabla, debug=False):\n",
    "        self.TABLA = tabla\n",
    "        self.entradas = [(regla[0], regla[1]) for regla in self.TABLA]   # Todas las entradas posibles\n",
    "        \n",
    "        \"\"\"Iniciamos los pesos y el umbral en un número decimal \n",
    "        aleatorio comprendido entre -10 y 10\"\"\"\n",
    "        self.PESOS = [random.uniform(-10, 10) for _ in range(3)] # Pesos para las dos entradas y el del umbral\n",
    "        self.UMBRAL = random.uniform(-10, 10) # Valor inicial del umbral\n",
    "        self.tasa_aprendizaje = tasa_aprendizaje\n",
    "        \n",
    "        self.n_iteracion = 0     # Vamos contando las iteraciones\n",
    "        self.aprendiendo = True  # Bandera para indicar que está aprendiendo la tabla\n",
    "        \n",
    "        self._debug = debug\n",
    "    \n",
    "    def funcion_de_activacion(self, valor):\n",
    "        \"\"\"Función que devuelve 1 si el valor provisto\n",
    "        supera el umbral, 0 en caso contrario.\"\"\"\n",
    "        return 1 if valor > self.UMBRAL else 0\n",
    "    \n",
    "    def salida_objetivo(self, entradas):\n",
    "        \"\"\"Obtiene la salida objetivo de la tabla de verdad\n",
    "        según el valor de las entradas provistas\"\"\"\n",
    "        for regla in self.TABLA:\n",
    "            if entradas[0] == regla[0] and entradas[1] == regla[1]:\n",
    "                return regla[2]\n",
    "    \n",
    "    def run(self):\n",
    "        while self.aprendiendo:  # Mientras no haya aprendido la tabla\n",
    "            self.n_iteracion += 1\n",
    "            self.aprendiendo = False\n",
    "            \n",
    "            if self._debug:\n",
    "                print(\"\\n--------------------------------\\nDEBUG: Iteración %d\\n\" % self.n_iteracion)\n",
    "            \n",
    "            for E in self.entradas:\n",
    "                P = self.PESOS\n",
    "                U = self.UMBRAL\n",
    "                PU = self.PESOS[2]  # Peso del umbral \n",
    "                salida_real = self.funcion_de_activacion(E[0]*P[0] + E[1]*P[1] + PU)\n",
    "                salida_objetivo = self.salida_objetivo(E)\n",
    "\n",
    "                ERROR = salida_objetivo - salida_real\n",
    "                if ERROR != 0:\n",
    "                    P[0] = P[0] + self.tasa_aprendizaje * ERROR * E[0]\n",
    "                    P[1] = P[1] + self.tasa_aprendizaje * ERROR * E[1]\n",
    "                    P[2] = P[2] + self.tasa_aprendizaje * ERROR\n",
    "                    self.aprendiendo = True\n",
    "                    \n",
    "                \n",
    "                if self._debug:\n",
    "                    print(\"Entradas = (%d, %d)\" % (E[0], E[1]))\n",
    "                    print(\"P_1 = %f\\tP_2 = %f\\tP_U = %f\" % (P[0], P[1], PU))\n",
    "                    print(\"UMBRAL = %f\" % U)\n",
    "                    print(\"Salida = %d\\tObjetivo = %d\\n\" % (salida_real, salida_objetivo))\n",
    "            if self._debug:            \n",
    "                print(\"--------------------------------\")\n",
    "            \n",
    "                \n",
    "        \n",
    "        print(\"Número de iteraciones necesarias: %d\" % self.n_iteracion)\n",
    "        \n",
    "\n",
    "print(\"TABLA DE VERDAD AND\")\n",
    "TABLA_AND = [        # TABLA DE VERDAD AND\n",
    "    [1, 1, 1],  # Verdadero y verdadero -> Verdadero\n",
    "    [1, 0, 0],  # Verdadero y falso     -> Falso\n",
    "    [0, 1, 0],  # Falso y verdadero     -> Falso\n",
    "    [0, 0, 0]   # Falso y falso         -> Falso\n",
    "]\n",
    "                \n",
    "ps = PerceptronSimple(0.3, TABLA_AND)\n",
    "ps.run()\n",
    "\n",
    "print(\"\\n\\n=======================================\\n\\n\")\n",
    "\n",
    "print(\"TABLA DE VERDAD OR\")\n",
    "TABLA_OR = [        # TABLA DE VERDAD OR\n",
    "    [1, 1, 1],  # Verdadero y verdadero -> Verdadero\n",
    "    [1, 0, 1],  # Verdadero y falso     -> Verdadero\n",
    "    [0, 1, 1],  # Falso y verdadero     -> Verdadero\n",
    "    [0, 0, 0]   # Falso y falso         -> Falso\n",
    "]\n",
    "\n",
    "ps = PerceptronSimple(0.3, TABLA_OR)\n",
    "ps.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________\n",
    "\n",
    "\n",
    "### <center>Siguiente capítulo: [**El perceptrón**](http://nbviewer.jupyter.org/github/mondeja/fullstack/blob/master/backend/src/analisis_de_datos/machine_learning/redes_neuronales/002-perceptron.ipynb) <center>\n",
    "    \n",
    "______________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Fuentes:\n",
    "- [Redes Neuronales parte 1 - Rafael Alberto Moreno Parra](https://openlibra.com/es/book/redes-neuronales-parte-1)\n",
    "- [Mutlilayer perceptron (Part 1) - The nature of code](https://www.youtube.com/watch?v=u5GAVdLQyIg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
